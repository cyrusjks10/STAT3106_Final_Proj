---
title: "Exploring Star Wars The Clone Wars episode scripts"
author: "Cyrus Jackson III"
date: "5/01/2022"
output: 
  html_document: 
    toc: yes
    theme: yeti
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!require("jsonlite")) install.packages("jsolinte")
if (!require("udpipe")) install.packages("udpipe")
if (!require("stopwords")) install.packages("stopwords")
if (!require("pscl")) install.packages("pscl")
if (!require("glmnet")) install.packages("glmnetl")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("stringr")) install.packages("stringr")

stopwds <- stopwords()
dl <- udpipe_download_model(language = "english")
eng_model <- udpipe_load_model(file = dl$file_model)

ep_scripts <-  read_json('ep_scripts.json')
eps_dat <- read.csv('eps_df.csv',stringsAsFactors = F)

```


# Background 

For this project, I will analyze the episode scripts of all 133 episodes covering all 7 seasons of the Star Wars the Clone Wars TV series, with the goal of uncovering insights about the preferences of viewers who watch the show. A central question that I seek to answer in this project is whether an episode's script is predictive of its rating. It seems to me that a script might be useful for predicting an episode's rating because an episode's script might reveal the characters that an episode features and the context in which an episode takes place. viewers might prefer some characters and settings over others, which might explain a nontrivial portion of the variability in the ratings between different episodes. 
A strategy analyst at Disney might be interested in uncovering insights about which characters and settings viewers of the Star Wars the Clone Wars TV series prefer because such insights might position them to better market toys and have a more informed idea about which future projects might satisfy Star Wars fans, which might help analysts to identify opportunities for revenue growth. Moreover, analyzing scripts of Star Wars the Clone Wars and other Star Wars media might provide additional data points for which characters and settings fans of the Star Wars franchise prefer, besides that of toy sales and social media fan content pertaining to Star Wars media. I am more interested in the question of whether it is possible to accurately predict an episode's rating based on its text and in the insights the prediction might uncover, being a fan of the show myself, though  I recognize the business value of this project.   

## Data Collection Process

For the purposes of this project, I will utilize two data sets in my analysis. The first dataset that I collected pertains to the IMDB ratings of each episode, which are submitted by viewers. The IMDB ratings of each episode was scraped from the IMDB website by using a python script, since I know of no API wrapper for R that could be used to retrieve the ratings for each episode. The second data set that I collected for the purposes of this project are the scripts of each episode, which I collected from the website https://subslikescript.com/series/Star_Wars_The_Clone_Wars-458290 . 

It is to my understanding that these scripts were compiled by fans of the show, since there was no mention of any automated technologies employed to collect the scripts nor any mention of the scripts being officially provided by the production company. Ideally, I would have preferred to use scripts that were provided by Lucasfilm (subsidiary of Disney), the company that produces the Star Wars Clone Wars TV Show, since such a data set would not just be official but also more standardized, which in being less likely to contain typos and errors and less likely to have inconsistencies in how the scripts are done and formatted might reduce the chance of their being any measurement error in the analysis. Episode scripts from sources where the scripts are crowd sourced by people passionate about the series are likely to have inconsistencies because multiple authors often contribute to putting together the scripts. 

## About the Data

The data pertaining to the IMDB ratings has 133 rows, which correspond the each episode in the tv series. 

The columns are:

`season`: The season that an episode belongs to
`episode_number` : The episode's episode number
`title` : The episode's title
`rating`; The episode's rating, which I will generally treat as a dependent variable for the prediction task that I am interested in.

```{r about the IMDB ratings data}
dim(eps_dat)
colnames(eps_dat)

```

The scripts data contain the scripts of each episode. The data is nearly cleaned, as all special characters have been removed and numbers have been replaced with 'number'. This was done via Python, as the data was being scraped and prepared for its analysis in R. head

```{r about the scripts data}
length(ep_scripts)
head(ep_scripts,1)
```

# EDA

## Data Cleaning
Before any analysis can be done on the episode scripts, the scripts need to be cleaned some more. For the pruposes of cleaning the scripts, I make sure all words are lowercase.

I will resume cleaning the text when I create our term frequency matrix, in preparation for building a model to predict en episode's ratings based on its script 
```{r}
lowercase_scripts <- tolower(ep_scripts)
```

## Exploring the data and its complexities 

I ask a few questions to help me get a sense of the complexities within the ratings data and how the ratings are are distributed with respect to the show's seasons and episodes. 

```{r}
# Which episode has the highest rating? What season does it belong to? 
eps_dat[which.max(eps_dat$rating),
c('episode_number','season')]

# What is that episode's title?
eps_dat[which.max(eps_dat$rating),'title']
```


```{r}
# Which episode has the loIst rating? What season does it belong to?
eps_dat[which.min(eps_dat$rating),
c('episode_number','season')]

# What is that episode's title?
eps_dat[which.min(eps_dat$rating),'title']
```


```{r}
# What is the median episode rating?
med_rating <- median(eps_dat$rating)
cat('\n The median episode rating is \n', med_rating)

# Which season has the highest median rating?
med_rtng_by_sn <- eps_dat %>% 
                  group_by(season) %>% 
                  summarise(median_rating_by_season =
                              median(rating))
med_rtng_by_sn[which.max(med_rtng_by_sn$median_rating_by_season),
               'season']

# Which season has the loIst median rating?
med_rtng_by_sn[which.min(med_rtng_by_sn$median_rating_by_season),'season']







```

I am also interested in the variability of episode ratings. If the episode ratings do not vary to a significant degree, then prediction might not be useful or meaningful.
```{r}

# How much do the ratings vary?
print(var(eps_dat$rating)) 
#The ratings vary almost by 7/10 of a point (on average) 
# For optimized algorithms, this is likely to be enough

# Which season has the highest variance of ratings?
var_rtngs_by_sn <- eps_dat %>% 
                  group_by(season) %>% 
                  summarise(variance_of_ratings_by_season =
                              var(rating))
var_rtngs_by_sn

var_rtngs_by_sn[which.max(var_rtngs_by_sn$variance_of_ratings_by_season),
                'season']


# Which season has the loIst variance of ratings?
var_rtngs_by_sn[which.min(var_rtngs_by_sn$variance_of_ratings_by_season),
                'season']
```


### Visualizing Data Complexities 

The ratings are distributed in a nearly normal manner, which suggests that regression is appropriate for the predictive task. 
```{r}
hist(eps_dat$rating,
     xlab = 'Rating',
     main='Distribution of ratings')
```

I explore plots to get a sense of how the episodes might relate to each other, in terms of their ratings. 

```{r}

# plot of episode rating by season
ggplot(eps_dat) + 
  geom_point(aes(x=episode_number,y=rating,color=factor(season)),size=2) +
  labs(x='Episode', y='Episode Rating') + ggtitle('Episode Rating by Season') +
  theme_classic()
```

Interestingly, it does not seem that the seasons of the show are clustered as strongly as one might think they would be in terms of the episode ratings. There does seem to be somewhat of a trend of later episodes in a season getting better ratings than earlier episodes in a season. This trend is really apparent in seasons 5, 6, and 7, though many of the seasons generally have higher ratings for later episodes than for earlier episodes.


```{r}

plot(x=var_rtngs_by_sn$season,
     y=var_rtngs_by_sn$variance_of_ratings_by_season,
     main= 'Variance of episode ratings by season',
     xlab='Season',
     ylab='Variance')
```

Season 7 has the most variance in the episode ratings of its season.

```{r}
numb_of_characters <- unlist(nchar(ep_scripts))
hist(numb_of_characters)

new_dat <- eps_dat

new_dat$numb_char <- numb_of_characters


# plot of number of characters in each episode script by season
ggplot(new_dat) + 
  geom_point(aes(x=episode_number,y=numb_char,color=factor(season)),size=2) +
  labs(x='Episode', y='Number of Characters') + ggtitle('Number of characters 
                                                        in episode script by
                                                        season') +
  theme_bw()

```
It seems that later episodes have scripts with less number of characters. There is also not very clear clustering in terms of the season. That there is not very much clustering in terms of the season suggests that there might be other important dimensions to cluster the data by, like the words that appear across all of the scripts. 


## Prepearing dataset for modeling

I am interested in the question of whether an episode's script can be helpful in predicting its rating, so I create a term frequency matrix to have a data set with the word counts of each word that occurs across all of the scripts. 

```{r message=FALSE, warning=FALSE}
eps_script_split <- str_split(lowercase_scripts, ".")
eps_script_split <- str_split(lowercase_scripts, "\n")


blob <- rep(NA, length(eps_script_split))
list_of_dfs <- vector(mode='list',length=length(eps_script_split))

for(i in seq_along(eps_script_split)){
  blob <-paste(eps_script_split[[i]], collapse = '')
  udpipe_out <- udpipe_annotate(eng_model,
                              x=blob,
                              tagger='default',
                              parser='none')
  lem_df <- as.data.frame(udpipe_out)
  word_counts_df <- as.data.frame(as.list(table(lem_df$lemma)))
  list_of_dfs[[i]] <- word_counts_df
  }

final_df <- bind_rows(list_of_dfs)

final_df1 <- final_df


dim(final_df)


# final_df[is.na(final_df)] <- 0
```

```{r}
head(final_df[,1:10])
```

I inspect how frequent each word is across the term frequency matrix that I created. It does not seem that a substantial proportion of words appear across all of the documents.
```{r}

perc_not_na <- apply(final_df1, 2, function(x) mean(!is.na(x)))
hist(
    perc_not_na, 
    xlab = "Frequency of word in documents",
    ylab = "Number of words",
    main = "Histogram of word frequency across documents"
)

```




# Modeling
I fit multiple models to predict an episode's rating based on its script, treating each word's occurrence as a feature in our model.

## Lasso Regression + Regression
The term frequency matrix that I created contains `r `ncol(final_df)`columns but only `r `nrow(final_df)` rows, meaning that I cannot utilize traditional regression methods, since such methods require that the number of rows not be exceed by the number of columns. 

I will utilize lasso regression, which is a kind of regression that shrinks as many coefficients as possible down to zero, allowing us to select the coefficients that do not have zero coefficients and use such coefficients in other models. I hope that this allows us to shrink down the number of features rtha

I first iterate through different lambda values, which control the extent to which the coefficients are reduced to zero, to find the value that minimizes the mean-squared error. The value of the shrinkage hyper parameter that does this is between the log(-4) and the log(-2)

```{r}
final_df[is.na(final_df)] <- 0

ls <- cv.glmnet(
    as.matrix(final_df), eps_dat$rating,
    alpha = 1, lambda = 10^seq(-5, 0, length.out=50)
)
plot(ls)

```

I identify the coefficients that are greater than zero. There are 50 of such coefficients. It seems that the lasso model was successful in shrinking down the number of features in our dataset. 

```{r}
k <- which(ls$lambda == ls$lambda.1se)
ls_coefs <- ls$glmnet.fit$beta[, k]
print(paste(
    'Proportion of 0 coefficients:', mean(ls_coefs == 0)
))

ls_coefs_top <- sort(abs(ls_coefs), decreasing = TRUE)[1:50]
print(names(ls_coefs_top[ls_coefs_top > 0]))



```

I utilize the coefficients that are not zero in a multiple regression model to see to what extent I can predict the episode's rating given the frequency of words in an episode's script.

```{r}
coefs_not_zero <- ls_coefs[ls_coefs!=0]

lasso_features <- names(coefs_not_zero)

x_matrix <- as.matrix(final_df[ , names(final_df) %in% lasso_features])

dim(x_matrix)

fit1 <- lm(eps_dat$rating~x_matrix)

summary(fit1)

plot(fit1)



```

The RSE was 0.2756. It does seem that one can predict an episode's rating based off of it's text. Moreover, the model explains nearly 96% of the variation of the ratings given the scripts' words. 

I see which words are associated with an increase in the ratings, as these words might provide insight into what star wars the clone wars fans like.

```{r}
coefs <- coef(fit1)
#coefs[coefs > 0]

sort(coefs[coefs > 0],decreasing=TRUE)
```

Because of the somewhat poor quality of the data, I was not able to make sense of all of the coefficients, though what stands out is that words like 'rex', 'maul,'krell', and 'darth' are estimated by the model to have a positive effect on the the score that an episode received. These coefficients that I have identified are all characters. The characters Rex and Krell are associated with clones, and since Star Wars the Clone Wars is supposed to be about the clone troopers fighting alongside Jedi, it makes sense that these words are estimated to increase the score that an episode received by the linear regression model. 

The words 'darth' and 'maul' are also estimated to have a positive effect on the rating that an episode received.These words are also associated with characters, specifically sith characters that are largely antagonists to the jedi and clones that the series mainly focus on. The fact that clones and sith characters, who often are engaged in battle when portrayed in the series, suggests that episodes that feature action scenes are preferred by vieIrs of the series.

This is further confirmed by the fact that 'treasonagainst', which I interpret to mean treason against, was featured in episodes with action packed scenes like the series finale *Victory and Death* (Season 7, episode 12) and *Carnage of Krell* (Season 4, episode 10). Not the mention that the coefficient 'jedi.' is also estimated to have a positive effect on an episode's ratings, and jedi are characters that nearly always appear before action scenes occur, since they are effectively warriors in the Star Wars' universe and often engage in fights and battles with other characters, usually alongside clones. 


Interestingly, some of the coefficients that the model estimates to have a positive effect on an episode's score are not necessarily associated with action scenes. Coefficients like 'jurisidiction', 'amidalas', and 'accord' are words that generally occur in episodes in which the politics of the CLone Wars universe are explored. I interpret 'amidalas' to refer to Senator Padme Amidala, who vocally opposes the war that the series explores. The terms jurisdictional and accord generally occur in episodes in which senators and other government officials give speeches. These episodes are often full of drama. These terms being estimated by the model to have a positive effect on a episode's ratings suggest that Star Wars fans might care to see (political drama), as Ill as action

I also examine which words are estimated by the model to have a negative effect on an episode's ratings.

```{r}

sort(coefs[coefs < 0],decreasing=F)
```

Terms like senator, amidala, representative, king, and agree are estimated to have a negative effect on an episode's rating, suggesting that Star Wars fans might find the politics of the Clone Wars universe somewhat polarizing, considering that earlier we learned that some words associated with Clone Wars universe politics were estimated by the regression model to have a positive effect on an episode's rating. This is not too suprising when considering that politics in the United States is often very polarizing. It seems that action scenes are a safer bet for appeasing Clone Wars fans than exploring the politics of the universe.



## Clustering

Our exploratory data analysis revealed that the ratings aren't clustered by seasons to the degree that you might expect. I explore other ways that the data might be clustered through a clustering algorithm. 

I create a term frequency–inverse document frequency for the purposes of clustering the data into different dimensions and normalziing the data such that the most frequent terms for a row are assinged a higher Iight than lesser used terms. 

```{r}


# tf_idf

#head(final_df1)
N <- nrow(final_df1)
nt <- apply(final_df1, 2, function(x) sum(!is.na(x)))


# idf
idf <- log(N/nt)
idf_mat <- matrix(rep(idf,N), byrow=T,ncol=length(idf))


# tf
tf <- final_df1/apply(final_df1,1,max, na.rm=T)

tfidf <- tf*idf_mat

class(tfidf)

dim(tfidf)

tfidf1 <- tfidf
head(tfidf[,1:10])
head(tfidf1[,1:10])



tfidf1[is.na(tfidf1)] <- 0
```

### Preparing dataset for clustering

I identify words that are stopwords, as these are words that are unlikely to be meaningful or convey information about the characters present in an episode nor the context that it occurs in. 

I remove words that seem as uninformative as stop words by removing the words with a maximum tdf_idf score across all of the documents lower than the maximum tdf-idf score of the stopwords. The tdf_idf score that we computed earlier effectively measure how important a word is to a particular document in showing the occurrence of a token normalized by the occurrence of the most frequently used token in the letter

```{r}

object_of_maxes <- apply(tfidf1, 2, max,na.rm=T)

lemm_token <- rep(NA,length(object_of_maxes))
max_tfidf_val<- rep(NA,length(object_of_maxes))

for(i in seq_along(object_of_maxes)){
 lemm_token[i] <- names(object_of_maxes[i])
  max_tfidf_val[i] <- object_of_maxes[[i]]
}

mtfidf_1 <- data.frame(lemm_token = lemm_token, max = max_tfidf_val)

maxes_stop_words <- mtfidf_1[mtfidf_1$lemm_token %in% stopwds,]

max_token <- maxes_stop_words[which.max(maxes_stop_words$max),]
max_token 

max_val <- max_token$max




impt_words_mtfidfs <- mtfidf_1[mtfidf_1$max >= max_val, ]



impt_words_mtfidfs_ord <- impt_words_mtfidfs[order(-impt_words_mtfidfs$max),]

top_tokens_df <- impt_words_mtfidfs_ord[1:40, ]
top_tokens <- top_tokens_df$lemm_token

top_tokens

```



We wonder if clustering can separate out episodes that have ratings higher than the median rating from episodes lower than the median ratings, which might indicate what Clone Wars fans have excitement about. 
```{r}

# create a dichotomous response variable
response_var <- ifelse(eps_dat$rating >= med_rating, 1,0) 

#response_var


# cluster text by kmeans algorithm with, forcing points ot be 
# assigned to two distinct groups

tfidf_clust <- tfidf1[,names(tfidf) %in% top_tokens]
head(tfidf_clust)

dist_mat <- dist(tfidf_clust)
h_out <- hclust(dist_mat, method = "ward.D")
plot(h_out)
h_clust <- cutree(h_out, k = 2)
```

To get a sense as to how the scripts are being clustered by the clustering algorithm, the top ten words with the largest TF_IDF values across the documents in a particular cluster will be examined. 

```{r}
for (i in 1:2) {
    mc_tfidf <- apply(tfidf_clust[h_clust == i,], 2, max)
    top_words <- tail(sort(mc_tfidf), 10)
    print(paste("Top words in cluster", i))
    print(names(top_words))    
}


table(h_clust,response_var)
```
The first cluster is mostly associated with action scenes, given that it is generally centered around words like trooper, pyke and zillo, which are references to characters and character types that occur in some of the series most action packed scenes.

The second cluster includes many terms that are also in the first cluster, indicating that the clustering algorithm was not able to effectively distinguish between episodes that scored above the median episode score and episodes that scored below the median episode rating based on the words with the highest 40 tf-idf values. Better results may have been achieveid if we selected the words to cluster on based on a lasso regression model and performance using a naive bayes classifier. 

# Summary/Conclusion

While our analysis suggests that viewers of the show prefer episodes filled with action and episodes thatmainly are about jedi, padawans, and clone troopers we must discuss the data's poor quality and how that likely impacted our analysis. Periods were placed between many words without a space and the spacing used to separate words was not consistent. It is possible that many of the words that we removed for having lower td-idf values than the stopwords were just misspelling of a similar group of words, for example jedi1 and jedii being removed though both semantically mean jedi. Revisiting this project, one might attempt to correct mispellings using cosine similarity or other distance measures in attempt to address the messy episode script data. Thus, we err on the side of caution and do not say for sure that future Star Wars content with more action scenes and scenes that feature jedi might appease fans more, given this potential measurement error. This analysis ought to be considered to be an additional data point in addition to the sales of action figures and tweets about the episodes that might be suggestive of star wars characters fans might prefer 


